{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d63096f",
   "metadata": {},
   "source": [
    "**PREPROCESSING**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ba30ad",
   "metadata": {},
   "source": [
    "Drop columns we don't need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09dab1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "col_to_drop = ['cc_num','trans_num','first','last','street']\n",
    "\n",
    "df_train = df_train.drop(columns=col_to_drop)\n",
    "df_test = df_test.drop(columns=col_to_drop)\n",
    "\n",
    "print(f\" Dropped {len(col_to_drop)} columns\")\n",
    "print(\"test columns : \\n \\n\",df_test.columns)\n",
    "print(\"\\n \\n train columns : \\n \\n\",df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949d2dc1",
   "metadata": {},
   "source": [
    "Add / Modify columns so that our model can understand the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387512dc",
   "metadata": {},
   "source": [
    "**TIME DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe73899",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add / Modify columns so that our model can understand the data \n",
    "\n",
    "# ----------------------------------------------------\n",
    "#                           TIME \n",
    "# ----------------------------------------------------\n",
    "\n",
    "# Convert to datetime type \n",
    "df_train['trans_date_trans_time'] = pd.to_datetime(df_train['trans_date_trans_time'])\n",
    "df_test['trans_date_trans_time'] = pd.to_datetime(df_test['trans_date_trans_time'])\n",
    "\n",
    "\n",
    "# Get hours (0 to 23)\n",
    "df_train['hour'] = df_train['trans_date_trans_time'].dt.hour\n",
    "df_test['hour'] = df_test['trans_date_trans_time'].dt.hour\n",
    "\n",
    "# Get day of week (0 = Monday / 6= Sunday)\n",
    "df_train['day_of_week'] = df_train['trans_date_trans_time'].dt.dayofweek\n",
    "df_test['day_of_week'] = df_test['trans_date_trans_time'].dt.dayofweek\n",
    "\n",
    "# Get day of month ( 1 to 31)\n",
    "df_train['day_of_month'] = df_train['trans_date_trans_time'].dt.day\n",
    "df_test['day_of_month'] = df_test['trans_date_trans_time'].dt.day\n",
    "\n",
    "# Get month (1 to 12)\n",
    "df_train['month'] = df_train['trans_date_trans_time'].dt.month\n",
    "df_test['month'] = df_test['trans_date_trans_time'].dt.month\n",
    "\n",
    "# Weekend or not ? \n",
    "df_train['is_weekend'] = (df_train['day_of_week'] >= 5).astype(int)\n",
    "df_test['is_weekend'] = (df_test['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Function to categrize time of day\n",
    "def get_time_period(hour):\n",
    "    \"\"\" \n",
    "    Categorize hour into time periods\n",
    "    0: Morning (low fraud risk)\n",
    "    1: Afternoon (low fraud risk)\n",
    "    2: Evening (moderate fraud risk)\n",
    "    3: Night (high fraud risk)\n",
    "    \"\"\"\n",
    "    if 5 <= hour < 12:\n",
    "        return 0\n",
    "    elif 12 <= hour < 18:\n",
    "        return 1\n",
    "    elif 18 <= hour < 23:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "# Apply \n",
    "df_train['time_period'] = df_train['hour'].apply(get_time_period)\n",
    "df_test['time_period'] = df_test['hour'].apply(get_time_period)\n",
    "\n",
    "# Analysis \n",
    "print(\"\\n Fraud transaction by hour: \")\n",
    "fraud_by_hour = df_train[df_train['is_fraud'] == 1].groupby('hour').size()\n",
    "print(fraud_by_hour.sort_values(ascending=False).head(5))\n",
    "\n",
    "# Distribution analysis\n",
    "print(\"\\n Distribution of 'hour' (fraud vs legitimate):\")\n",
    "print(\"\\nFraud transactions by hour:\")\n",
    "fraud_by_hour = df_train[df_train['is_fraud'] == 1].groupby('hour').size()\n",
    "print(fraud_by_hour.sort_values(ascending=False).head(5))\n",
    "\n",
    "print(\"\\n Fraud rate by time period:\")\n",
    "for period, name in [(0, 'Morning'), (1, 'Afternoon'), (2, 'Evening'), (3, 'Night')]:\n",
    "    total = (df_train['time_period'] == period).sum()\n",
    "    frauds = ((df_train['time_period'] == period) & (df_train['is_fraud'] == 1)).sum()\n",
    "    rate = (frauds / total * 100) if total > 0 else 0\n",
    "    print(f\"   {name}: {frauds:,} frauds / {total:,} transactions = {rate:.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e72c23",
   "metadata": {},
   "source": [
    "**DISTANCE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea42f53a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "geographic_columns = ['lat','long','merch_lat','merch_long']\n",
    "\n",
    "# Compute the distance between client and merchant\n",
    "diff_lat_train = df_train['lat'] - df_train['merch_lat']\n",
    "diff_long_train = df_train['long'] - df_train['merch_long']\n",
    "\n",
    "# Pythagorean theorem (in degrees because of coordinates)\n",
    "dist_degree_train = np.sqrt(diff_lat_train**2 + diff_long_train**2)\n",
    "dist_km_train = dist_degree_train * 111   # Approximation, 1° is about 111km \n",
    "df_train['distance_km'] = dist_km_train\n",
    "\n",
    "\n",
    "# Same for test dataset \n",
    "diff_lat_test = df_test['lat'] - df_test['merch_lat']\n",
    "diff_long_test = df_test['long'] - df_test['merch_long']\n",
    "dist_degree_test = np.sqrt(diff_lat_test**2 + diff_long_test**2)\n",
    "dist_km_test = dist_degree_test * 111  \n",
    "df_test['distance_km'] = dist_km_test\n",
    "\n",
    "# Overall statistics\n",
    "print(\"\\n Distance statistics (TRAIN):\")\n",
    "print(f\"   Mean: {df_train['distance_km'].mean():.2f} km\")\n",
    "print(f\"   Median: {df_train['distance_km'].median():.2f} km\")\n",
    "print(f\"   Std Dev: {df_train['distance_km'].std():.2f} km\")\n",
    "print(f\"   Min: {df_train['distance_km'].min():.2f} km\")\n",
    "print(f\"   Max: {df_train['distance_km'].max():.2f} km\")\n",
    "print(f\"  ligne 2: {df_train['distance_km'][1]}\")\n",
    "print(f\" Merchant : {df_train['merchant'][1]}\")\n",
    "\n",
    "# Percentiles\n",
    "print(\"\\n Distance percentiles:\")\n",
    "percentiles = [25, 50, 75, 90, 95, 99]\n",
    "for p in percentiles:\n",
    "    value = df_train['distance_km'].quantile(p/100)\n",
    "    print(f\"   {p}th percentile: {value:.2f} km\")\n",
    "\n",
    "# Distance by fraud status\n",
    "print(\"\\n Distance by class:\")\n",
    "for fraud_status in [0, 1]:\n",
    "    label = \"Legitimate\" if fraud_status == 0 else \"Fraud\"\n",
    "    subset = df_train[df_train['is_fraud'] == fraud_status]['distance_km']\n",
    "    print(f\"\\n   {label} transactions:\")\n",
    "    print(f\"      Mean: {subset.mean():.2f} km\")\n",
    "    print(f\"      Median: {subset.median():.2f} km\")\n",
    "    print(f\"      75th percentile: {subset.quantile(0.75):.2f} km\")\n",
    "    print(f\"      95th percentile: {subset.quantile(0.95):.2f} km\")\n",
    "\n",
    "# Distance categories\n",
    "print(\"\\n Distance categories distribution:\")\n",
    "def categorize_distance(dist):\n",
    "    if dist < 10:\n",
    "        return \"Very Close (<10 km)\"\n",
    "    elif dist < 50:\n",
    "        return \"Close (10-50 km)\"\n",
    "    elif dist < 200:\n",
    "        return \"Regional (50-200 km)\"\n",
    "    elif dist < 500:\n",
    "        return \"Far (200-500 km)\"\n",
    "    else:\n",
    "        return \"Very Far (>500 km)\"\n",
    "\n",
    "df_train['distance_category'] = df_train['distance_km'].apply(categorize_distance)\n",
    "\n",
    "for category in [\"Very Close (<10 km)\", \"Close (10-50 km)\", \"Regional (50-200 km)\", \n",
    "                \"Far (200-500 km)\", \"Very Far (>500 km)\"]:\n",
    "    total = (df_train['distance_category'] == category).sum()\n",
    "    frauds = ((df_train['distance_category'] == category) & (df_train['is_fraud'] == 1)).sum()\n",
    "    fraud_rate = (frauds / total * 100) if total > 0 else 0\n",
    "    print(f\"   {category}: {frauds:,} frauds / {total:,} = {fraud_rate:.3f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# 4. VISUALIZATIONS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"4. CREATING VISUALIZATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Distance distribution (histogram)\n",
    "axes[0, 0].hist(df_train['distance_km'], bins=100, color='steelblue', \n",
    "               edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Distance (km)', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[0, 0].set_title('Distance Distribution (All Transactions)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlim(0, df_train['distance_km'].quantile(0.99))  # Remove extreme outliers\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Distance distribution by fraud status (overlay)\n",
    "for fraud_status, label, color in [(0, 'Legitimate', 'green'), (1, 'Fraud', 'red')]:\n",
    "    subset = df_train[df_train['is_fraud'] == fraud_status]['distance_km']\n",
    "    axes[0, 1].hist(subset, bins=100, alpha=0.6, label=label, color=color, edgecolor='black')\n",
    "\n",
    "axes[0, 1].set_xlabel('Distance (km)', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "axes[0, 1].set_title('Distance Distribution by Class', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].set_xlim(0, df_train['distance_km'].quantile(0.99))\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Boxplot by fraud status\n",
    "df_train.boxplot(column='distance_km', by='is_fraud', ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Class', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Distance (km)', fontsize=11, fontweight='bold')\n",
    "axes[1, 0].set_title('Distance by Class (Boxplot)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].get_figure().suptitle('')\n",
    "plt.sca(axes[1, 0])\n",
    "plt.xticks([1, 2], ['Legitimate', 'Fraud'])\n",
    "axes[1, 0].set_ylim(0, df_train['distance_km'].quantile(0.99))\n",
    "\n",
    "# Plot 4: Fraud rate by distance category\n",
    "distance_cats = [\"Very Close (<10 km)\", \"Close (10-50 km)\", \"Regional (50-200 km)\", \n",
    "                \"Far (200-500 km)\", \"Very Far (>500 km)\"]\n",
    "fraud_rates = []\n",
    "for cat in distance_cats:\n",
    "    total = (df_train['distance_category'] == cat).sum()\n",
    "    frauds = ((df_train['distance_category'] == cat) & (df_train['is_fraud'] == 1)).sum()\n",
    "    fraud_rates.append((frauds / total * 100) if total > 0 else 0)\n",
    "\n",
    "axes[1, 1].bar(range(len(distance_cats)), fraud_rates, color='crimson', \n",
    "              edgecolor='black', linewidth=1.5)\n",
    "axes[1, 1].set_xlabel('Distance Category', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Fraud Rate (%)', fontsize=11, fontweight='bold')\n",
    "axes[1, 1].set_title('Fraud Rate by Distance Category', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xticks(range(len(distance_cats)))\n",
    "axes[1, 1].set_xticklabels(['<10km', '10-50km', '50-200km', '200-500km', '>500km'], \n",
    "                           rotation=45, ha='right')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add values on bars\n",
    "for i, rate in enumerate(fraud_rates):\n",
    "    axes[1, 1].text(i, rate, f'{rate:.3f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('geographic_distance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\n Visualization saved: geographic_distance_analysis.png\")\n",
    "plt.show()\n",
    "\n",
    "# Drop temporary category column\n",
    "df_train = df_train.drop(columns=['distance_category'])\n",
    "\n",
    "# ============================================================================\n",
    "# 5. SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "print(\"\\n What we created:\")\n",
    "print(\"   • distance_km: Euclidean distance between customer and merchant\")\n",
    "\n",
    "print(f\"\\n Key findings:\")\n",
    "print(f\"   • Mean distance (Legitimate): {df_train[df_train['is_fraud']==0]['distance_km'].mean():.2f} km\")\n",
    "print(f\"   • Mean distance (Fraud): {df_train[df_train['is_fraud']==1]['distance_km'].mean():.2f} km\")\n",
    "\n",
    "print(f\"\\n Current shapes:\")\n",
    "print(f\"   TRAIN: {df_train.shape}\")\n",
    "print(f\"   TEST: {df_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fdf657",
   "metadata": {},
   "source": [
    "**DEMOGRAPHIC DATA (AGE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efd851d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert to datetime\n",
    "df_train['dob_datetime'] = pd.to_datetime(df_train['dob'])\n",
    "df_test['dob_datetime'] = pd.to_datetime(df_test['dob'])\n",
    "\n",
    "# Compute age (in years)\n",
    "df_train['age'] = (df_train['trans_datetime'] - df_train['dob_datetime']).dt.days / 365.25\n",
    "df_test['age'] = (df_test['trans_datetime'] - df_test['dob_datetime']).dt.days / 365.25\n",
    "\n",
    "# Clip to make the data more realistic \n",
    "df_train['age'] = df_train['age'].clip(lower=18,upper=100)\n",
    "df_test['age'] = df_test['age'].clip(lower=18,upper=100)\n",
    "\n",
    "\n",
    "df_train = df_train.drop(columns=['dob'])\n",
    "df_test = df_test.drop(columns=['dob'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def categorize_age(age):\n",
    "    if 18 <= age <= 25:\n",
    "        return \"Young (18-25)\"\n",
    "    if 25 < age <= 50:\n",
    "        return \"Adults (25-50)\"\n",
    "    else: \n",
    "        return \"Old (>50)\"\n",
    "    \n",
    "# Apply \n",
    "df_train['age_category'] = df_train['age'].apply(categorize_age)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Age statistics :\")\n",
    "print(df_train['age'].describe())\n",
    "\n",
    "print(\"\\nDistribution :\")\n",
    "print(df_train['age_category'].value_counts())\n",
    "\n",
    "print(\"\\nMean age for each category :\")\n",
    "print(df_train.groupby('is_fraud')['age'].agg(['mean', 'median', 'std']).round(2))\n",
    "\n",
    "print(\"\\nFraud rate by category :\")\n",
    "fraud_rate = df_train.groupby('age_category')['is_fraud'].agg(['mean']) * 100\n",
    "print(fraud_rate.round(2))\n",
    "\n",
    "# Drop columns we don't need anymore \n",
    "columns_to_drop = ['trans_date_trans_time']\n",
    "\n",
    "df_train = df_train.drop(columns=columns_to_drop)\n",
    "df_test = df_test.drop(columns=columns_to_drop)\n",
    "\n",
    "fraud_by_cat = df_train.groupby('age_category')['is_fraud'].mean() * 100\n",
    "fraud_by_cat.plot(kind='bar', color=['#ff9999', '#66b3ff', '#99ff99'])\n",
    "plt.title(\"Taux de fraude (%) par catégorie d'âge\")\n",
    "plt.ylabel(\"Taux de fraude (%)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64045930",
   "metadata": {},
   "source": [
    "**OTHER DATA ENCODING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b0958",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Identify categorical columns \n",
    "categorical_columns = df_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\n Categorical columns : {len(categorical_columns)}\")\n",
    "for col in categorical_columns:\n",
    "    nunique = df_train[col].nunique()\n",
    "    print(f\" •{col}: {nunique:,} unique values\")\n",
    "\n",
    "\n",
    "# Gender (M/F -> 1/0) (Label Encoding)\n",
    "if 'gender' in df_train.columns:\n",
    "    le_gender = LabelEncoder()\n",
    "    df_train['gender'] = le_gender.fit_transform(df_train['gender'])\n",
    "    df_test['gender'] = le_gender.transform(df_test['gender'])\n",
    "\n",
    "    print(f\"\\n Gender encoded:\")\n",
    "    print(f\"{le_gender.classes_[0]} -> 0\")\n",
    "    print(f\"{le_gender.classes_[1]} -> 1\")\n",
    "\n",
    "    print(f\"\\n After encoding:\")\n",
    "    print(df_train['gender'].value_counts())\n",
    "else:\n",
    "    print(\"\\n 'gender' column not found\")\n",
    "\n",
    "\n",
    "# Target Encoding for high cardinality columns\n",
    "high_card_cols = ['category', 'merchant', 'job']\n",
    "\n",
    "for col in high_card_cols:\n",
    "    if col in df_train.columns:\n",
    "        fraud_rates = df_train.groupby(col)['is_fraud'].mean()\n",
    "        global_mean = df_train['is_fraud'].mean()\n",
    "\n",
    "        df_train[f'{col}_encoded'] = df_train[col].map(fraud_rates)\n",
    "        df_test[f'{col}_encoded'] = df_test[col].map(fraud_rates).fillna(global_mean)\n",
    "\n",
    "        # Show top 5 encoded values\n",
    "        print(f\"   Top 5 {col} by fraud rate:\")\n",
    "        top_fraud_rates = fraud_rates.sort_values(ascending=False).head(5)\n",
    "        for cat, rate in top_fraud_rates.items():\n",
    "            print(f\"      {cat}: {rate:.4f}\")\n",
    "\n",
    "        # Drop original column\n",
    "        df_train = df_train.drop(columns=[col])\n",
    "        df_test = df_test.drop(columns=[col])\n",
    "\n",
    "\n",
    "# State encoding\n",
    "if 'state' in df_train.columns:\n",
    "    n_states = df_train['state'].nunique()\n",
    "\n",
    "    if n_states <= 60:\n",
    "        # One-hot encoding\n",
    "        df_train = pd.get_dummies(df_train, columns=['state'], prefix='state', drop_first=False)\n",
    "        df_test = pd.get_dummies(df_test, columns=['state'], prefix='state', drop_first=False)\n",
    "\n",
    "        # Align columns (in case test has different states)\n",
    "        train_cols = set(df_train.columns)\n",
    "        test_cols = set(df_test.columns)\n",
    "\n",
    "        # Add missing columns to test\n",
    "        for col in train_cols - test_cols:\n",
    "            df_test[col] = 0\n",
    "\n",
    "        # Remove extra columns from test\n",
    "        for col in test_cols - train_cols:\n",
    "            df_test = df_test.drop(columns=[col])\n",
    "\n",
    "        # Reorder test columns to match train\n",
    "        df_test = df_test[df_train.columns]\n",
    "    else:\n",
    "        # Target encode if too many states \n",
    "        fraud_rates = df_train.groupby('state')['is_fraud'].mean()\n",
    "        global_mean = df_train['is_fraud'].mean()\n",
    "\n",
    "        df_train['state_encoded'] = df_train['state'].map(fraud_rates)\n",
    "        df_test['state_encoded'] = df_test['state'].map(fraud_rates).fillna(global_mean)\n",
    "\n",
    "        df_train = df_train.drop(columns=['state'])\n",
    "        df_test = df_test.drop(columns=['state'])\n",
    "\n",
    "\n",
    "# Check for any remaining object columns\n",
    "remaining_cats = df_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if remaining_cats:\n",
    "    print(f\"\\n Found {len(remaining_cats)} remaining categorical columns:\")\n",
    "    for col in remaining_cats:\n",
    "        nunique = df_train[col].nunique()\n",
    "        print(f\"   • {col}: {nunique:,} unique values\")\n",
    "\n",
    "    print(\"\\n Applying Target Encoding to remaining columns...\")\n",
    "    for col in remaining_cats:\n",
    "        fraud_rates = df_train.groupby(col)['is_fraud'].mean()\n",
    "        global_mean = df_train['is_fraud'].mean()\n",
    "\n",
    "        df_train[f'{col}_encoded'] = df_train[col].map(fraud_rates)\n",
    "        df_test[f'{col}_encoded'] = df_test[col].map(fraud_rates).fillna(global_mean)\n",
    "\n",
    "        df_train = df_train.drop(columns=[col])\n",
    "        df_test = df_test.drop(columns=[col])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e2d10c",
   "metadata": {},
   "source": [
    "**VERIFICATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44af4dea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Check data types\n",
    "print(f\"\\n Data types after encoding:\")\n",
    "print(df_train.dtypes.value_counts())\n",
    "\n",
    "# Verify no object columns remain\n",
    "object_cols = df_train.select_dtypes(include=['object']).columns.tolist()\n",
    "if object_cols:\n",
    "    print(f\"\\n WARNING: {len(object_cols)} object columns still present: {object_cols}\")\n",
    "else:\n",
    "    print(f\"\\All columns are numeric!\")\n",
    "\n",
    "print(f\"\\n Current shapes:\")\n",
    "print(f\"   TRAIN: {df_train.shape}\")\n",
    "print(f\"   TEST: {df_test.shape}\")\n",
    "\n",
    "print(f\"\\n Current columns ({len(df_train.columns)}):\")\n",
    "print(df_train.columns.tolist()[:20])  # Show first 20\n",
    "if len(df_train.columns) > 20:\n",
    "    print(f\"   ... and {len(df_train.columns) - 20} more\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cd6b64",
   "metadata": {},
   "source": [
    "**CORRECTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b0fb67",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Remove remaining datetime columns\n",
    "datetime_cols = df_train.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "if datetime_cols:\n",
    "    print(f\"\\n Removing {len(datetime_cols)} datetime columns:\")\n",
    "    for col in datetime_cols:\n",
    "        print(f\"    {col}\")\n",
    "    df_train = df_train.drop(columns=datetime_cols)\n",
    "    df_test = df_test.drop(columns=datetime_cols)\n",
    "    print(\"    Datetime columns removed\")\n",
    "else:\n",
    "    print(\"\\n No datetime columns to remove\")\n",
    "\n",
    "# 2. Convert bool columns to int (for state_* columns)\n",
    "bool_cols = df_train.select_dtypes(include=['bool']).columns.tolist()\n",
    "if bool_cols:\n",
    "    print(f\"\\n Converting {len(bool_cols)} boolean columns to int:\")\n",
    "    df_train[bool_cols] = df_train[bool_cols].astype(int)\n",
    "    df_test[bool_cols] = df_test[bool_cols].astype(int)\n",
    "    print(f\"    {len(bool_cols)} columns converted (bool -> int)\")\n",
    "else:\n",
    "    print(\"\\n No boolean columns to convert\")\n",
    "\n",
    "# 3. Verify all columns are numeric\n",
    "\n",
    "print(f\"\\n Final data types:\")\n",
    "print(df_train.dtypes.value_counts())\n",
    "\n",
    "# Check for non-numeric columns\n",
    "non_numeric = df_train.select_dtypes(exclude=['int64', 'int32', 'float64']).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(f\"\\n WARNING: {len(non_numeric)} non-numeric columns found:\")\n",
    "    for col in non_numeric:\n",
    "        print(f\"   • {col}: {df_train[col].dtype}\")\n",
    "else:\n",
    "    print(f\"\\n SUCCESS: All columns are numeric!\")\n",
    "\n",
    "print(f\"\\n Final shapes:\")\n",
    "print(f\"   TRAIN: {df_train.shape}\")\n",
    "print(f\"   TEST: {df_test.shape}\")\n",
    "\n",
    "# Show column summary\n",
    "print(f\"\\n Column breakdown:\")\n",
    "print(f\"   • Target variable: is_fraud\")\n",
    "print(f\"   • Numeric features: {df_train.shape[1] - 1}\")\n",
    "print(f\"   • Total columns: {df_train.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6b7dc",
   "metadata": {},
   "source": [
    "**CHECK IF THERE IS DATA LEAKAGE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e95ab1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if 'merchant' in df_train.columns:\n",
    "    fraud_merchants = df_train['merchant'].str.contains('fraud', case=False, na=False).sum()\n",
    "    print(f\"   Merchants with 'fraud' in name: {fraud_merchants}\")\n",
    "    if fraud_merchants > 0:\n",
    "        print(\"    WARNING: Potential data leakage!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168174cd",
   "metadata": {},
   "source": [
    "**FEATURE SCALING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0f113",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X_train = df_train.drop('is_fraud', axis=1)\n",
    "y_train = df_train['is_fraud']\n",
    "\n",
    "X_test = df_test.drop('is_fraud', axis=1)\n",
    "y_test = df_test['is_fraud']\n",
    "\n",
    "print(f\"\\n Separated features and target:\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "print(f\"   X_test: {X_test.shape}\")\n",
    "print(f\"   y_test: {y_test.shape}\")\n",
    "\n",
    "\n",
    "# Get all numeric columns\n",
    "all_cols = X_train.columns.tolist()\n",
    "\n",
    "# Identify binary columns (0/1 only) - don't need scaling\n",
    "binary_cols = []\n",
    "for col in all_cols:\n",
    "    unique_vals = X_train[col].nunique()\n",
    "    if unique_vals == 2:\n",
    "        vals = set(X_train[col].unique())\n",
    "        if vals.issubset({0, 1, 0.0, 1.0}):\n",
    "            binary_cols.append(col)\n",
    "\n",
    "print(f\"   Binary columns (no scaling needed): {len(binary_cols)}\")\n",
    "print(f\"   Examples: {binary_cols[:5]}\")\n",
    "\n",
    "# Columns to scale: all except binary\n",
    "cols_to_scale = [col for col in all_cols if col not in binary_cols]\n",
    "print(f\"\\n   Columns to scale: {len(cols_to_scale)}\")\n",
    "print(f\"   Examples: {cols_to_scale[:10]}\")\n",
    "\n",
    "# Show before scaling (first 3 rows, selected columns)\n",
    "print(f\"\\n Before scaling (sample):\")\n",
    "sample_cols = ['amt', 'lat', 'age', 'distance_km']\n",
    "existing_sample_cols = [col for col in sample_cols if col in X_train.columns]\n",
    "print(X_train[existing_sample_cols].head(3))\n",
    "\n",
    "# Initialize and fit scaler on TRAIN data only\n",
    "print(\"\\n Fitting StandardScaler on TRAIN data...\")\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[cols_to_scale])\n",
    "\n",
    "# Transform both train and test\n",
    "X_train[cols_to_scale] = scaler.transform(X_train[cols_to_scale])\n",
    "X_test[cols_to_scale] = scaler.transform(X_test[cols_to_scale])\n",
    "\n",
    "\n",
    "# Show after scaling\n",
    "print(f\"\\n After scaling (same sample):\")\n",
    "print(X_train[existing_sample_cols].head(3))\n",
    "\n",
    "print(\"\\n Verify scaling (should be ~0 mean, ~1 std):\")\n",
    "for col in existing_sample_cols:\n",
    "    if col in cols_to_scale:\n",
    "        print(f\"   {col}: mean={X_train[col].mean():.6f}, std={X_train[col].std():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26539178",
   "metadata": {},
   "source": [
    "**TRAIN / VALIDATION SPLIT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0fbce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Split train into train_final and validation\n",
    "X_train_final, X_val, y_train_final, y_val = train_test_split(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_train  # CRITICAL: maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"\\n Split completed:\")\n",
    "print(f\"   Train (final): {X_train_final.shape} - for training models\")\n",
    "print(f\"   Validation:    {X_val.shape} - for hyperparameter tuning\")\n",
    "print(f\"   Test:          {X_test.shape} - for final evaluation (DON'T TOUCH!)\")\n",
    "\n",
    "# Verify class distribution\n",
    "print(f\"\\n Class distribution verification:\")\n",
    "train_fraud_rate = y_train_final.mean() * 100\n",
    "val_fraud_rate = y_val.mean() * 100\n",
    "test_fraud_rate = y_test.mean() * 100\n",
    "\n",
    "print(f\"   Train:      {y_train_final.sum():,} frauds / {len(y_train_final):,} = {train_fraud_rate:.3f}%\")\n",
    "print(f\"   Validation: {y_val.sum():,} frauds / {len(y_val):,} = {val_fraud_rate:.3f}%\")\n",
    "print(f\"   Test:       {y_test.sum():,} frauds / {len(y_test):,} = {test_fraud_rate:.3f}%\")\n",
    "\n",
    "if abs(train_fraud_rate - val_fraud_rate) < 0.01 and abs(train_fraud_rate - test_fraud_rate) < 0.01:\n",
    "    print(\"    Distributions are consistent!\")\n",
    "else:\n",
    "    print(\"    Warning: Distributions differ slightly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13299be0",
   "metadata": {},
   "source": [
    "**HANDLING OF CLASS IMBALANCE (USING SMOTE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc4bee0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n The Problem:\")\n",
    "print(f\"   Current ratio: ~{int(1/train_fraud_rate*100)}:1 (legitimate:fraud)\")\n",
    "print(\"   Without balancing:\")\n",
    "print(\"   • Model will predict 'legitimate' for everything\")\n",
    "print(\"   • Accuracy = 99.4% but 0 frauds detected!\")\n",
    "print(\"   • Useless in production\")\n",
    "\n",
    "print(\"\\n Solution: SMOTE (Synthetic Minority Over-sampling Technique)\")\n",
    "print(\"   • Creates synthetic fraud examples by interpolation\")\n",
    "print(\"   • Interpolates between existing fraud transactions\")\n",
    "print(\"   • sampling_strategy=0.3 → target ratio 3:1 (not 1:1, too artificial)\")\n",
    "\n",
    "print(\"\\n CRITICAL RULES:\")\n",
    "print(\"   1. Apply SMOTE ONLY on train_final (NOT on validation/test!)\")\n",
    "print(\"   2. Never touch validation/test sets (data leakage!)\")\n",
    "print(\"   3. Validation/test must reflect real-world distribution\")\n",
    "\n",
    "# Show before SMOTE\n",
    "print(f\"\\n BEFORE SMOTE:\")\n",
    "print(f\"   Class 0 (legitimate): {(y_train_final == 0).sum():,}\")\n",
    "print(f\"   Class 1 (fraud):      {(y_train_final == 1).sum():,}\")\n",
    "print(f\"   Ratio: {(y_train_final == 0).sum() / (y_train_final == 1).sum():.1f}:1\")\n",
    "\n",
    "# Apply SMOTE\n",
    "print(\"\\n Applying SMOTE...\")\n",
    "smote = SMOTE(\n",
    "    sampling_strategy=0.3,  # Target: 30% fraud (ratio ~3:1)\n",
    "    random_state=42,\n",
    "    k_neighbors=5\n",
    ")\n",
    "\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_final, y_train_final)\n",
    "\n",
    "\n",
    "# Show after SMOTE\n",
    "print(f\"\\n AFTER SMOTE:\")\n",
    "print(f\"   Class 0 (legitimate): {(y_train_resampled == 0).sum():,}\")\n",
    "print(f\"   Class 1 (fraud):      {(y_train_resampled == 1).sum():,}\")\n",
    "print(f\"   Ratio: {(y_train_resampled == 0).sum() / (y_train_resampled == 1).sum():.1f}:1\")\n",
    "\n",
    "print(f\"\\n Size changes:\")\n",
    "print(f\"   Before: {X_train_final.shape[0]:,} samples\")\n",
    "print(f\"   After:  {X_train_resampled.shape[0]:,} samples\")\n",
    "print(f\"   Added:  {X_train_resampled.shape[0] - X_train_final.shape[0]:,} synthetic frauds\")\n",
    "\n",
    "# Convert back to DataFrame (SMOTE returns numpy arrays)\n",
    "X_train_resampled = pd.DataFrame(X_train_resampled, columns=X_train_final.columns)\n",
    "y_train_resampled = pd.Series(y_train_resampled, name='is_fraud')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
